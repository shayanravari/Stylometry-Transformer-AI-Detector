{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b9e31ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, precision_recall_curve, average_precision_score, roc_curve, auc, confusion_matrix\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel, pipeline\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import textstat\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c112c270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_nb(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "def preprocess_text_bert_input(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"<[^>]+>\", \" \", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "def load_dev_data_from_single_file(filepath):\n",
    "    texts, labels, prompts = [], [], []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                \n",
    "                if data.get(\"human_text\"):\n",
    "                    texts.append(data[\"human_text\"])\n",
    "                    labels.append(0)\n",
    "                    prompts.append(data.get(\"prompt\", \"N/A\"))\n",
    "\n",
    "                if data.get(\"machine_text\"):\n",
    "                    texts.append(data[\"machine_text\"])\n",
    "                    labels.append(1)\n",
    "                    prompts.append(data.get(\"prompt\", \"N/A\"))\n",
    "            except Exception: continue\n",
    "    return texts, labels, prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99a552d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_NAMES = ['flesch_reading_ease', 'flesch_kincaid_grade', 'gunning_fog', 'smog_index', 'automated_readability_index', 'coleman_liau_index', 'lexicon_count', 'sentence_count', 'avg_sentence_length', 'avg_word_length', 'type_token_ratio']\n",
    "\n",
    "def extract_stylometric_features(text: str) -> np.ndarray:\n",
    "    if not text or len(text.split()) < 3: return np.zeros(len(FEATURE_NAMES))\n",
    "    try:\n",
    "        features = [\n",
    "            textstat.flesch_reading_ease(text),\n",
    "            textstat.flesch_kincaid_grade(text),\n",
    "            textstat.gunning_fog(text),\n",
    "            textstat.smog_index(text),\n",
    "            textstat.automated_readability_index(text),\n",
    "            textstat.coleman_liau_index(text),\n",
    "            textstat.lexicon_count(text),\n",
    "            textstat.sentence_count(text),\n",
    "            textstat.avg_sentence_length(text),\n",
    "            textstat.avg_word_length(text),\n",
    "        ]\n",
    "        words = text.lower().split(); ttr = len(set(words)) / len(words) if len(words) > 0 else 0\n",
    "        features.append(ttr)\n",
    "        return np.array(features, dtype=np.float32)\n",
    "    except Exception: return np.zeros(len(FEATURE_NAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2737b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_data(texts, labels, prompts, model_name_prefix, preds, probs, output_dir=\"evaluation_outputs\"):\n",
    "    print(f\"\\nEvaluating {model_name_prefix}\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\", pos_label=1, zero_division=0)\n",
    "    metrics = {'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1}\n",
    "    print(f\"Results for {model_name_prefix} → Acc: {acc:.4f}, Prec: {prec:.4f}, Rec: {rec:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted Human', 'Predicted AI'], yticklabels=['Actual Human', 'Actual AI'])\n",
    "    plt.title(f'Confusion Matrix for {model_name_prefix}')\n",
    "    plt.ylabel('Actual Label'); plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f\"{model_name_prefix}_confusion_matrix.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # ROC Curve\n",
    "    if len(np.unique(labels)) > 1:\n",
    "        fpr, tpr, _ = roc_curve(labels, probs, pos_label=1)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        metrics['roc_auc'] = roc_auc\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})'); plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC for {model_name_prefix}')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f\"{model_name_prefix}_roc_curve.png\"))\n",
    "        plt.close()\n",
    "    else: metrics['roc_auc'] = np.nan\n",
    "\n",
    "    # Precision-Recall Curve\n",
    "    if len(np.unique(labels)) > 1:\n",
    "        precision_vals, recall_vals, _ = precision_recall_curve(labels, probs, pos_label=1)\n",
    "        ap = average_precision_score(labels, probs, pos_label=1)\n",
    "        metrics['average_precision'] = ap\n",
    "        plt.figure(); plt.plot(recall_vals, precision_vals, lw=2, color='blue', label=f'PR curve (AP = {ap:.2f})')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.title(f'Precision-Recall Curve for {model_name_prefix}')\n",
    "        plt.legend(loc=\"best\"); plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f\"{model_name_prefix}_pr_curve.png\"))\n",
    "        plt.close()\n",
    "    else: metrics['average_precision'] = np.nan\n",
    "\n",
    "    # Distribution of Prediction Scores\n",
    "    probs_human_true = [probs[i] for i, label in enumerate(labels) if label == 0]\n",
    "    probs_ai_true = [probs[i] for i, label in enumerate(labels) if label == 1]\n",
    "    plt.figure()\n",
    "    if probs_human_true: sns.histplot(probs_human_true, bins=30, alpha=0.6, label='True Human', color='skyblue', kde=False)\n",
    "    if probs_ai_true: sns.histplot(probs_ai_true, bins=30, alpha=0.6, label='True AI', color='salmon', kde=False)\n",
    "    plt.title(f'Distribution of Predicted AI Probabilities ({model_name_prefix})')\n",
    "    plt.xlabel('Predicted Probability of Being AI-Generated')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f\"{model_name_prefix}_prob_distribution.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    misclassified_examples = []\n",
    "    for i in range(len(texts)):\n",
    "        if preds[i] != labels[i]:\n",
    "            misclassified_examples.append({\"prompt\": prompts[i], \"text\": texts[i], \"true_label\": \"Human\" if labels[i] == 0 else \"AI\", \"predicted_label\": \"Human\" if preds[i] == 0 else \"AI\", \"prob_ai_generated\": probs[i]})\n",
    "    return metrics, misclassified_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7bf5971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "DEV_SET_ROOT_DIR = \"./devset/\"\n",
    "OUTPUT_DIR = \"evaluation_outputs_final\"\n",
    "BERT_MODEL_DIR = \"./bert_ai_detector_final\"\n",
    "NAIVE_BAYES_MODEL_PATH = \"baseline_saved_model/naive_bayes_model.joblib\"\n",
    "NAIVE_BAYES_VECTORIZER_PATH = \"baseline_saved_model/tfidf_vectorizer.joblib\"\n",
    "HYBRID_CLASSIFIER_PATH = \"hybrid_model/hybrid_classifier.joblib\"\n",
    "SCALER_PATH = \"hybrid_model/feature_scaler.joblib\"\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(f\"Using device: {'cuda:0' if device == 0 else 'cpu'}.\")\n",
    "\n",
    "# Load All Models\n",
    "nb_clf = joblib.load(NAIVE_BAYES_MODEL_PATH)\n",
    "tfidf_vec = joblib.load(NAIVE_BAYES_VECTORIZER_PATH)\n",
    "bert_pipeline = pipeline(\"text-classification\", model=BERT_MODEL_DIR, tokenizer=BERT_MODEL_DIR, device=device)\n",
    "hybrid_clf = joblib.load(HYBRID_CLASSIFIER_PATH)\n",
    "scaler = joblib.load(SCALER_PATH)\n",
    "bert_base_model = AutoModel.from_pretrained(BERT_MODEL_DIR).to(device).eval()\n",
    "bert_base_tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d462e959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Processing Dev File: arxiv_chatGPT\n",
      "\n",
      "Evaluating NaiveBayes_arxiv_chatGPT\n",
      "Results for NaiveBayes_arxiv_chatGPT → Acc: 0.5095, Prec: 0.5718, Rec: 0.0757, F1: 0.1336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shaya\\Desktop\\CS 162 Final Project\\.env\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating BERT_arxiv_chatGPT\n",
      "Results for BERT_arxiv_chatGPT → Acc: 0.9555, Prec: 0.9627, Rec: 0.9477, F1: 0.9551\n",
      "\n",
      "Extracting hybrid features for arxiv_chatGPT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hybrid BERT Features: 100%|██████████| 6000/6000 [01:24<00:00, 71.37it/s]\n",
      "C:\\Users\\shaya\\AppData\\Local\\Temp\\ipykernel_10196\\2556657146.py:15: DeprecationWarning: The 'avg_sentence_length' method has been deprecated due to being the same as 'words_per_sentence'. This method will be removed in thefuture.\n",
      "  textstat.avg_sentence_length(text),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Hybrid_BERT_arxiv_chatGPT\n",
      "Results for Hybrid_BERT_arxiv_chatGPT → Acc: 0.9502, Prec: 0.9409, Rec: 0.9607, F1: 0.9507\n",
      "\n",
      "\n",
      "Processing Dev File: arxiv_cohere\n",
      "\n",
      "Evaluating NaiveBayes_arxiv_cohere\n",
      "Results for NaiveBayes_arxiv_cohere → Acc: 0.6597, Prec: 0.8690, Rec: 0.3760, F1: 0.5249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shaya\\Desktop\\CS 162 Final Project\\.env\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating BERT_arxiv_cohere\n",
      "Results for BERT_arxiv_cohere → Acc: 0.8843, Prec: 0.9565, Rec: 0.8053, F1: 0.8744\n",
      "\n",
      "Extracting hybrid features for arxiv_cohere...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hybrid BERT Features: 100%|██████████| 6000/6000 [01:21<00:00, 73.57it/s]\n",
      "C:\\Users\\shaya\\AppData\\Local\\Temp\\ipykernel_10196\\2556657146.py:15: DeprecationWarning: The 'avg_sentence_length' method has been deprecated due to being the same as 'words_per_sentence'. This method will be removed in thefuture.\n",
      "  textstat.avg_sentence_length(text),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Hybrid_BERT_arxiv_cohere\n",
      "Results for Hybrid_BERT_arxiv_cohere → Acc: 0.8963, Prec: 0.9339, Rec: 0.8530, F1: 0.8916\n",
      "\n",
      "\n",
      "Processing Dev File: reddit_chatGPT\n",
      "\n",
      "Evaluating NaiveBayes_reddit_chatGPT\n",
      "Results for NaiveBayes_reddit_chatGPT → Acc: 0.4757, Prec: 0.4657, Rec: 0.3300, F1: 0.3863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shaya\\Desktop\\CS 162 Final Project\\.env\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating BERT_reddit_chatGPT\n",
      "Results for BERT_reddit_chatGPT → Acc: 0.8777, Prec: 0.9926, Rec: 0.7610, F1: 0.8615\n",
      "\n",
      "Extracting hybrid features for reddit_chatGPT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hybrid BERT Features: 100%|██████████| 6000/6000 [01:26<00:00, 69.69it/s]\n",
      "C:\\Users\\shaya\\AppData\\Local\\Temp\\ipykernel_10196\\2556657146.py:15: DeprecationWarning: The 'avg_sentence_length' method has been deprecated due to being the same as 'words_per_sentence'. This method will be removed in thefuture.\n",
      "  textstat.avg_sentence_length(text),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Hybrid_BERT_reddit_chatGPT\n",
      "Results for Hybrid_BERT_reddit_chatGPT → Acc: 0.9252, Prec: 0.9822, Rec: 0.8660, F1: 0.9205\n",
      "\n",
      "\n",
      "Processing Dev File: reddit_cohere\n",
      "\n",
      "Evaluating NaiveBayes_reddit_cohere\n",
      "Results for NaiveBayes_reddit_cohere → Acc: 0.5998, Prec: 0.3699, Rec: 0.5467, F1: 0.4413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shaya\\Desktop\\CS 162 Final Project\\.env\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating BERT_reddit_cohere\n",
      "Results for BERT_reddit_cohere → Acc: 0.9164, Prec: 0.9811, Rec: 0.7246, F1: 0.8336\n",
      "\n",
      "Extracting hybrid features for reddit_cohere...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hybrid BERT Features: 100%|██████████| 4220/4220 [00:57<00:00, 72.91it/s]\n",
      "C:\\Users\\shaya\\AppData\\Local\\Temp\\ipykernel_10196\\2556657146.py:15: DeprecationWarning: The 'avg_sentence_length' method has been deprecated due to being the same as 'words_per_sentence'. This method will be removed in thefuture.\n",
      "  textstat.avg_sentence_length(text),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Hybrid_BERT_reddit_cohere\n",
      "Results for Hybrid_BERT_reddit_cohere → Acc: 0.9374, Prec: 0.9552, Rec: 0.8221, F1: 0.8837\n",
      "\n",
      "\n",
      "Overall Dev Set Performance Summary (F1-Score):\n",
      "dataset      arxiv_chatGPT  arxiv_cohere  reddit_chatGPT  reddit_cohere\n",
      "model                                                                  \n",
      "BERT                0.9551        0.8744          0.8615         0.8336\n",
      "Hybrid_BERT         0.9507        0.8916          0.9205         0.8837\n",
      "NaiveBayes          0.1336        0.5249          0.3863         0.4413\n",
      "\n",
      "\n",
      "Average Performance Across Dev Subsets:\n",
      "             accuracy  precision  recall      f1  roc_auc  average_precision\n",
      "model                                                                       \n",
      "BERT           0.9085     0.9732  0.8096  0.8812   0.9762             0.9761\n",
      "Hybrid_BERT    0.9273     0.9531  0.8754  0.9116   0.9811             0.9778\n",
      "NaiveBayes     0.5611     0.5691  0.3321  0.3715   0.6028             0.5493\n"
     ]
    }
   ],
   "source": [
    "dev_filenames = [\"arxiv_chatGPT.jsonl\", \"arxiv_cohere.jsonl\", \"reddit_chatGPT.jsonl\", \"reddit_cohere.jsonl\"]\n",
    "ethics_set_filenames = [\"german_wikipedia.jsonl\", \"hewlett.json\", \"toefl.json\"]\n",
    "all_results_summary = []\n",
    "\n",
    "for filename in dev_filenames:\n",
    "    filepath = os.path.join(DEV_SET_ROOT_DIR, filename)\n",
    "    dataset_short_name = filename.replace(\".jsonl\", \"\")\n",
    "    print(f\"\\n\\nProcessing Dev File: {dataset_short_name}\")\n",
    "    texts, labels, prompts = load_dev_data_from_single_file(filepath)\n",
    "\n",
    "    # Evaluate Naive Bayes\n",
    "    nb_processed_texts = [preprocess_text_nb(t) for t in texts]\n",
    "    X_dev_tfidf = tfidf_vec.transform(nb_processed_texts)\n",
    "    nb_preds = nb_clf.predict(X_dev_tfidf)\n",
    "    nb_probs = nb_clf.predict_proba(X_dev_tfidf)[:, 1]\n",
    "    nb_metrics, nb_errors = evaluate_model_on_data(texts, labels, prompts, f\"NaiveBayes_{dataset_short_name}\", nb_preds, nb_probs, OUTPUT_DIR)\n",
    "    all_results_summary.append({**nb_metrics, 'model': 'NaiveBayes', 'dataset': dataset_short_name})\n",
    "\n",
    "    # Evaluate BERT\n",
    "    bert_processed_texts = [preprocess_text_bert_input(t) for t in texts]\n",
    "    bert_preds_raw = bert_pipeline(bert_processed_texts, truncation=True, padding=True, max_length=256, return_all_scores=True)\n",
    "    positive_class_label_str = 'LABEL_1'\n",
    "    if bert_pipeline.model.config.id2label[1]:\n",
    "        positive_class_label_str = bert_pipeline.model.config.id2label[1]\n",
    "\n",
    "    bert_probs = [p['score'] for d in bert_preds_raw for p in d if p['label'] == positive_class_label_str]\n",
    "    bert_preds = [np.argmax([p['score'] for p in d]) for d in bert_preds_raw]\n",
    "    bert_metrics, bert_errors = evaluate_model_on_data(texts, labels, prompts, f\"BERT_{dataset_short_name}\", bert_preds, bert_probs, OUTPUT_DIR)\n",
    "    all_results_summary.append({**bert_metrics, 'model': 'BERT', 'dataset': dataset_short_name})\n",
    "\n",
    "    # Evaluate Hybrid Model\n",
    "    print(f\"\\nExtracting hybrid features for {dataset_short_name}...\")\n",
    "    bert_feats_dev = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Hybrid BERT Features\"):\n",
    "            inputs = bert_base_tokenizer(preprocess_text_bert_input(text), return_tensors=\"pt\", truncation=True, max_length=256).to(device)\n",
    "            outputs = bert_base_model(**inputs)\n",
    "            bert_feats_dev.append(outputs.last_hidden_state[:, 0, :].cpu().numpy().squeeze())\n",
    "            \n",
    "    style_feats_dev = np.array([extract_stylometric_features(t) for t in texts])\n",
    "    X_hybrid_dev = np.concatenate([np.array(bert_feats_dev), style_feats_dev], axis=1)\n",
    "    X_hybrid_dev_scaled = scaler.transform(X_hybrid_dev)\n",
    "    \n",
    "    hybrid_preds = hybrid_clf.predict(X_hybrid_dev_scaled)\n",
    "    hybrid_probs = hybrid_clf.predict_proba(X_hybrid_dev_scaled)[:, 1]\n",
    "    hybrid_metrics, hybrid_errors = evaluate_model_on_data(texts, labels, prompts, f\"Hybrid_BERT_{dataset_short_name}\", hybrid_preds, hybrid_probs, OUTPUT_DIR)\n",
    "    all_results_summary.append({**hybrid_metrics, 'model': 'Hybrid_BERT', 'dataset': dataset_short_name})\n",
    "\n",
    "# Final Results\n",
    "if all_results_summary:\n",
    "    results_df = pd.DataFrame(all_results_summary)\n",
    "    print(\"\\n\\nOverall Dev Set Performance Summary (F1-Score):\")\n",
    "    print(results_df.pivot_table(index='model', columns='dataset', values='f1').round(4))\n",
    "    print(\"\\n\\nAverage Performance Across Dev Subsets:\")\n",
    "    avg_performance = results_df.groupby('model')[['accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'average_precision']].mean()\n",
    "    print(avg_performance.round(4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b9e31ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, precision_recall_curve, average_precision_score, roc_curve, auc\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "import torch\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d462e959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes model and vectorizer loaded successfully.\n",
      "Using device: cuda:0 for BERT.\n",
      "\n",
      "\n",
      "--- Processing Dev File: ./devset/arxiv_chatGPT.jsonl ---\n",
      "\n",
      "Evaluating NaiveBayes_arxiv_chatGPT...\n",
      "Results for NaiveBayes_arxiv_chatGPT → Acc: 0.5095, Prec: 0.5718, Rec: 0.0757, F1: 0.1336\n",
      "Top 3 NB Misclassified for arxiv_chatGPT:\n",
      "  NB_Err 1: True=AI, Pred=Human, Text='In this paper, we investigate the continuum limit of polymer quantum mechanics. The aim of our work ...'\n",
      "  NB_Err 2: True=AI, Pred=Human, Text='In this paper, we present the results of our analysis of the Serpens star-forming region using data ...'\n",
      "  NB_Err 3: True=AI, Pred=Human, Text='In this work, we present a new method of integrating stochastic differential equations on Lie groups...'\n",
      "\n",
      "Evaluating BERT_arxiv_chatGPT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shaya\\Desktop\\CS 162 Final Project\\.env\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for BERT_arxiv_chatGPT → Acc: 0.8642, Prec: 0.7987, Rec: 0.9737, F1: 0.8776\n",
      "Top 3 BERT Misclassified for arxiv_chatGPT:\n",
      "  BERT_Err 1: True=Human, Pred=AI, Text='  A rather non-standard quantum representation of the canonical commutation\n",
      "relations of quantum mec...'\n",
      "  BERT_Err 2: True=Human, Pred=AI, Text='  We present Lie group integrators for nonlinear stochastic differential\n",
      "equations with non-commutat...'\n",
      "  BERT_Err 3: True=Human, Pred=AI, Text='  The multisite phosphorylation-dephosphorylation cycle is a motif repeatedly\n",
      "used in cell signaling...'\n",
      "\n",
      "\n",
      "--- Processing Dev File: ./devset/arxiv_cohere.jsonl ---\n",
      "\n",
      "Evaluating NaiveBayes_arxiv_cohere...\n",
      "Results for NaiveBayes_arxiv_cohere → Acc: 0.6597, Prec: 0.8690, Rec: 0.3760, F1: 0.5249\n",
      "Top 3 NB Misclassified for arxiv_cohere:\n",
      "  NB_Err 1: True=AI, Pred=Human, Text='\n",
      "We consider a system of many polymers in solution that interact via an external force that is appli...'\n",
      "  NB_Err 2: True=AI, Pred=Human, Text='\n",
      "\n",
      "Spectroscopic Observations of the Intermediate Polar EX Hydrae in Quiescence\n",
      "\n",
      "We present a photome...'\n",
      "  NB_Err 3: True=AI, Pred=Human, Text='\n",
      "\n",
      "ALMA as the ideal probe of the solar chromosphere\n",
      "We present a detailed study of the solar chromos...'\n",
      "\n",
      "Evaluating BERT_arxiv_cohere...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shaya\\Desktop\\CS 162 Final Project\\.env\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for BERT_arxiv_cohere → Acc: 0.8147, Prec: 0.7810, Rec: 0.8747, F1: 0.8252\n",
      "Top 3 BERT Misclassified for arxiv_cohere:\n",
      "  BERT_Err 1: True=Human, Pred=AI, Text='  A rather non-standard quantum representation of the canonical commutation\n",
      "relations of quantum mec...'\n",
      "  BERT_Err 2: True=AI, Pred=Human, Text='\n",
      "\n",
      "We present a catalog of 66 YSOs in the Serpens cloud, as observed with IRAC and MIPS, and discuss ...'\n",
      "  BERT_Err 3: True=Human, Pred=AI, Text='  We present Lie group integrators for nonlinear stochastic differential\n",
      "equations with non-commutat...'\n",
      "\n",
      "\n",
      "--- Processing Dev File: ./devset/reddit_chatGPT.jsonl ---\n",
      "\n",
      "Evaluating NaiveBayes_reddit_chatGPT...\n",
      "Results for NaiveBayes_reddit_chatGPT → Acc: 0.4757, Prec: 0.4657, Rec: 0.3300, F1: 0.3863\n",
      "Top 3 NB Misclassified for reddit_chatGPT:\n",
      "  NB_Err 1: True=Human, Pred=AI, Text='In 1801, James Monroe and Robert R. Livingston (the R. also stood for Robert, oddly enough) were sen...'\n",
      "  NB_Err 2: True=Human, Pred=AI, Text='Good question! [I answered this a few years back,](_URL_7_) but it was a fun topic so I'll pull that...'\n",
      "  NB_Err 3: True=AI, Pred=Human, Text='The practice of removing pubic hair is not a modern fad, as many historical cultures have engaged in...'\n",
      "\n",
      "Evaluating BERT_reddit_chatGPT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shaya\\Desktop\\CS 162 Final Project\\.env\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for BERT_reddit_chatGPT → Acc: 0.8938, Prec: 0.8267, Rec: 0.9967, F1: 0.9037\n",
      "Top 3 BERT Misclassified for reddit_chatGPT:\n",
      "  BERT_Err 1: True=Human, Pred=AI, Text='Henry died in a joust against the captain of his Scottish Guard, Gabriel, the Count of Montgomery.  ...'\n",
      "  BERT_Err 2: True=Human, Pred=AI, Text='Watergate is an incredibly interesting period of political history that I feel is greatly misunderst...'\n",
      "  BERT_Err 3: True=Human, Pred=AI, Text='No, in medieval Europe,  there were no restrictions on common soldiers killing a king or any other a...'\n",
      "\n",
      "\n",
      "--- Processing Dev File: ./devset/reddit_cohere.jsonl ---\n",
      "\n",
      "Evaluating NaiveBayes_reddit_cohere...\n",
      "Results for NaiveBayes_reddit_cohere → Acc: 0.5998, Prec: 0.3699, Rec: 0.5467, F1: 0.4413\n",
      "Top 3 NB Misclassified for reddit_cohere:\n",
      "  NB_Err 1: True=AI, Pred=Human, Text='\n",
      "\n",
      "The English king Henry II was famously involved in a 1559 jousting accident in which he accidental...'\n",
      "  NB_Err 2: True=Human, Pred=AI, Text='In 1801, James Monroe and Robert R. Livingston (the R. also stood for Robert, oddly enough) were sen...'\n",
      "  NB_Err 3: True=Human, Pred=AI, Text='Good question! [I answered this a few years back,](_URL_7_) but it was a fun topic so I'll pull that...'\n",
      "\n",
      "Evaluating BERT_reddit_cohere...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shaya\\Desktop\\CS 162 Final Project\\.env\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for BERT_reddit_cohere → Acc: 0.8457, Prec: 0.6561, Rec: 0.9803, F1: 0.7861\n",
      "Top 3 BERT Misclassified for reddit_cohere:\n",
      "  BERT_Err 1: True=Human, Pred=AI, Text='Henry died in a joust against the captain of his Scottish Guard, Gabriel, the Count of Montgomery.  ...'\n",
      "  BERT_Err 2: True=Human, Pred=AI, Text='Watergate is an incredibly interesting period of political history that I feel is greatly misunderst...'\n",
      "  BERT_Err 3: True=Human, Pred=AI, Text='No, in medieval Europe,  there were no restrictions on common soldiers killing a king or any other a...'\n",
      "\n",
      "\n",
      "--- Overall Dev Set Performance Summary ---\n",
      "        model         dataset  accuracy  precision    recall        f1\n",
      "0  NaiveBayes   arxiv_chatGPT  0.509500   0.571788  0.075667  0.133647\n",
      "1        BERT   arxiv_chatGPT  0.864167   0.798742  0.973667  0.877572\n",
      "2  NaiveBayes    arxiv_cohere  0.659667   0.869029  0.376000  0.524895\n",
      "3        BERT    arxiv_cohere  0.814667   0.780952  0.874667  0.825157\n",
      "4  NaiveBayes  reddit_chatGPT  0.475667   0.465663  0.330000  0.386266\n",
      "5        BERT  reddit_chatGPT  0.893833   0.826652  0.996667  0.903733\n",
      "6  NaiveBayes   reddit_cohere  0.599763   0.369939  0.546721  0.441283\n",
      "7        BERT   reddit_cohere  0.845735   0.656061  0.980328  0.786066\n",
      "\n",
      "\n",
      "--- Average Performance Across Dev Subsets (per model) ---\n",
      "            accuracy  precision    recall        f1\n",
      "model                                              \n",
      "BERT        0.854600   0.765602  0.956332  0.848132\n",
      "NaiveBayes  0.561149   0.569105  0.332097  0.371523\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text_nb(text): # For Naive Bayes\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
    "    toks = text.split()\n",
    "    return \" \".join(toks)\n",
    "\n",
    "def preprocess_text_bert_input(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"<[^>]+>\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def load_dev_data_from_single_file(filepath):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    processed_prompts = []\n",
    "    count = 0\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                human_text = data.get(\"human_text\")\n",
    "                machine_text = data.get(\"machine_text\")\n",
    "                prompt = data.get(\"prompt\", \"N/A\") # Get prompt if available\n",
    "\n",
    "                if human_text and isinstance(human_text, str) and len(human_text.strip()) > 0:\n",
    "                    texts.append(human_text)\n",
    "                    labels.append(0)\n",
    "                    processed_prompts.append(prompt)\n",
    "                \n",
    "                if machine_text and isinstance(machine_text, str) and len(machine_text.strip()) > 0:\n",
    "                    texts.append(machine_text)\n",
    "                    labels.append(1)\n",
    "                    processed_prompts.append(prompt)\n",
    "                count +=1\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Warning: Could not decode JSON from line in {filepath}: {line.strip()}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error processing line {count} in {filepath}: {line.strip()} - {e}\")\n",
    "    return texts, labels, processed_prompts\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate_model_on_data(texts, labels, prompts, model_name_prefix, model_obj,\n",
    "                           is_bert=False, bert_tokenizer_obj=None, nb_vectorizer_obj=None,\n",
    "                           output_dir=\"evaluation_outputs\"):\n",
    "\n",
    "    print(f\"\\nEvaluating {model_name_prefix}...\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    all_preds = []\n",
    "    all_probs_positive_class = []\n",
    "\n",
    "    if is_bert:\n",
    "        bert_pipeline = model_obj\n",
    "        id2label = bert_pipeline.model.config.id2label\n",
    "        label_to_int = {v: k for k, v in id2label.items()}\n",
    "        positive_class_label_str = None\n",
    "        for int_val, str_val in id2label.items():\n",
    "            if int_val == 1:\n",
    "                positive_class_label_str = str_val\n",
    "                break\n",
    "        if positive_class_label_str is None:\n",
    "            if 1 in id2label:\n",
    "                 positive_class_label_str = id2label[1]\n",
    "            else:\n",
    "                if len(id2label) > 1:\n",
    "                     positive_class_label_str = list(id2label.values())[1]\n",
    "\n",
    "\n",
    "        processed_texts_for_bert = [preprocess_text_bert_input(t) for t in texts]\n",
    "        chunk_size = 32\n",
    "        for i in range(0, len(processed_texts_for_bert), chunk_size):\n",
    "            batch_texts = processed_texts_for_bert[i:i+chunk_size]\n",
    "            raw_batch_preds_detailed = bert_pipeline(batch_texts, truncation=True, padding=True, max_length=256, return_all_scores=True)\n",
    "\n",
    "            for item_preds_detailed in raw_batch_preds_detailed:\n",
    "                current_pred_label_str = \"\"\n",
    "                max_score = -1.0\n",
    "                \n",
    "                prob_positive = 0.0\n",
    "                for pred_info in item_preds_detailed:\n",
    "                    if pred_info['label'] == positive_class_label_str:\n",
    "                        prob_positive = pred_info['score']\n",
    "                    if pred_info['score'] > max_score:\n",
    "                        max_score = pred_info['score']\n",
    "                        current_pred_label_str = pred_info['label']\n",
    "                \n",
    "                all_preds.append(label_to_int[current_pred_label_str])\n",
    "                all_probs_positive_class.append(prob_positive)\n",
    "        preds = all_preds\n",
    "\n",
    "    else:\n",
    "        processed_texts_for_nb = [preprocess_text_nb(t) for t in texts]\n",
    "        X_dev_tfidf = nb_vectorizer_obj.transform(processed_texts_for_nb)\n",
    "        nb_classifier = model_obj\n",
    "        preds = nb_classifier.predict(X_dev_tfidf)\n",
    "        probs = nb_classifier.predict_proba(X_dev_tfidf)\n",
    "        all_probs_positive_class = probs[:, 1].tolist() # Prob of class 1\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\", pos_label=1, zero_division=0)\n",
    "    \n",
    "    metrics = {'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1}\n",
    "\n",
    "    misclassified_examples = []\n",
    "    for i in range(len(texts)):\n",
    "        if preds[i] != labels[i]:\n",
    "            misclassified_examples.append({\n",
    "                \"prompt\": prompts[i],\n",
    "                \"text\": texts[i],\n",
    "                \"text_length_words\": len(texts[i].split()),\n",
    "                \"true_label\": \"Human\" if labels[i] == 0 else \"AI\",\n",
    "                \"predicted_label\": \"Human\" if preds[i] == 0 else \"AI\",\n",
    "                \"prob_ai_generated\": all_probs_positive_class[i] # Prob of being AI\n",
    "            })\n",
    "\n",
    "    print(f\"Results for {model_name_prefix} → Acc: {acc:.4f}, Prec: {prec:.4f}, Rec: {rec:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Predicted Human', 'Predicted AI'], \n",
    "                yticklabels=['Actual Human', 'Actual AI'])\n",
    "    plt.title(f'Confusion Matrix for {model_name_prefix}')\n",
    "    plt.ylabel('Actual Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f\"{model_name_prefix}_confusion_matrix.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # ROC Curve\n",
    "    if len(np.unique(labels)) > 1:\n",
    "        fpr, tpr, _ = roc_curve(labels, all_probs_positive_class, pos_label=1)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        metrics['roc_auc'] = roc_auc\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC for {model_name_prefix}')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f\"{model_name_prefix}_roc_curve.png\"))\n",
    "        plt.close()\n",
    "    else:\n",
    "        metrics['roc_auc'] = np.nan\n",
    "\n",
    "\n",
    "    # Precision-Recall Curve\n",
    "    if len(np.unique(labels)) > 1:\n",
    "        precision_vals, recall_vals, _ = precision_recall_curve(labels, all_probs_positive_class, pos_label=1)\n",
    "        ap = average_precision_score(labels, all_probs_positive_class, pos_label=1)\n",
    "        metrics['average_precision'] = ap\n",
    "        plt.figure()\n",
    "        plt.plot(recall_vals, precision_vals, lw=2, color='blue', label=f'PR curve (AP = {ap:.2f})')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.title(f'Precision-Recall Curve for {model_name_prefix}')\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f\"{model_name_prefix}_pr_curve.png\"))\n",
    "        plt.close()\n",
    "    else:\n",
    "        metrics['average_precision'] = np.nan\n",
    "\n",
    "\n",
    "    # Distribution of Prediction Scores\n",
    "    probs_human_true = [all_probs_positive_class[i] for i, label in enumerate(labels) if label == 0]\n",
    "    probs_ai_true = [all_probs_positive_class[i] for i, label in enumerate(labels) if label == 1]\n",
    "    plt.figure()\n",
    "    if probs_human_true:\n",
    "        sns.histplot(probs_human_true, bins=30, alpha=0.6, label='True Human', color='skyblue', kde=False)\n",
    "    if probs_ai_true:\n",
    "        sns.histplot(probs_ai_true, bins=30, alpha=0.6, label='True AI', color='salmon', kde=False)\n",
    "    plt.title(f'Distribution of Predicted AI Probabilities ({model_name_prefix})')\n",
    "    plt.xlabel('Predicted Probability of Being AI-Generated')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f\"{model_name_prefix}_prob_distribution.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    return metrics, misclassified_examples\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    DEV_SET_ROOT_DIR = \"./devset/\"\n",
    "    NAIVE_BAYES_MODEL_PATH = \"baseline_saved_model/naive_bayes_model.joblib\" \n",
    "    NAIVE_BAYES_VECTORIZER_PATH = \"baseline_saved_model/tfidf_vectorizer.joblib\"\n",
    "    BERT_MODEL_DIR = \"./bert_ai_detector_final\"\n",
    "\n",
    "    # Load Naive Bayes Model and Vectorizer\n",
    "    nb_clf_loaded = None\n",
    "    tfidf_vec_loaded = None\n",
    "\n",
    "    nb_clf_loaded = joblib.load(NAIVE_BAYES_MODEL_PATH)\n",
    "    tfidf_vec_loaded = joblib.load(NAIVE_BAYES_VECTORIZER_PATH)\n",
    "\n",
    "    # Load BERT Model and Tokenizer\n",
    "    bert_pipeline_loaded = None\n",
    "    device = 0 if torch.cuda.is_available() else -1\n",
    "    print(f\"Using device: {'cuda:0' if device == 0 else 'cpu'} for BERT.\")\n",
    "\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_DIR)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(BERT_MODEL_DIR)\n",
    "        \n",
    "    bert_pipeline_loaded = pipeline(\n",
    "            \"text-classification\", \n",
    "            model=model, \n",
    "            tokenizer=tokenizer, \n",
    "            device=device\n",
    "    )\n",
    "\n",
    "    dev_filenames = [\n",
    "        \"arxiv_chatGPT.jsonl\",\n",
    "        \"arxiv_cohere.jsonl\",\n",
    "        \"reddit_chatGPT.jsonl\",\n",
    "        \"reddit_cohere.jsonl\"\n",
    "    ]\n",
    "\n",
    "    all_results_summary = [] # To store dicts for final DataFrame\n",
    "\n",
    "    for filename in dev_filenames:\n",
    "        filepath = os.path.join(DEV_SET_ROOT_DIR, filename)\n",
    "        dataset_short_name = filename.replace(\".jsonl\", \"\")\n",
    "        print(f\"\\n\\n--- Processing Dev File: {filepath} ---\")\n",
    "        texts, labels, prompts = load_dev_data_from_single_file(filepath)\n",
    "        \n",
    "        # Evaluate Naive Bayes\n",
    "        if nb_clf_loaded and tfidf_vec_loaded:\n",
    "            nb_metrics, nb_errors = evaluate_model_on_data(\n",
    "                texts, labels, prompts, \n",
    "                model_name_prefix=f\"NaiveBayes_{dataset_short_name}\", \n",
    "                model_obj=nb_clf_loaded, \n",
    "                is_bert=False, \n",
    "                nb_vectorizer_obj=tfidf_vec_loaded\n",
    "            )\n",
    "            nb_metrics['model'] = f\"NaiveBayes\"\n",
    "            nb_metrics['dataset'] = dataset_short_name\n",
    "            all_results_summary.append(nb_metrics)\n",
    "            print(f\"Top 3 NB Misclassified for {dataset_short_name}:\")\n",
    "            for i, err in enumerate(nb_errors[:3]):\n",
    "                 print(f\"  NB_Err {i+1}: True={err['true_label']}, Pred={err['predicted_label']}, Text='{err['text'][:100]}...'\")\n",
    "\n",
    "        if bert_pipeline_loaded:\n",
    "            bert_metrics, bert_errors = evaluate_model_on_data(\n",
    "                texts, labels, prompts,\n",
    "                model_name_prefix=f\"BERT_{dataset_short_name}\", \n",
    "                model_obj=bert_pipeline_loaded, \n",
    "                is_bert=True\n",
    "            )\n",
    "            bert_metrics['model'] = f\"BERT\"\n",
    "            bert_metrics['dataset'] = dataset_short_name\n",
    "            all_results_summary.append(bert_metrics)\n",
    "            print(f\"Top 3 BERT Misclassified for {dataset_short_name}:\")\n",
    "            for i, err in enumerate(bert_errors[:3]):\n",
    "                 print(f\"  BERT_Err {i+1}: True={err['true_label']}, Pred={err['predicted_label']}, Text='{err['text'][:100]}...'\")\n",
    "\n",
    "\n",
    "    # Print Results Table\n",
    "    if all_results_summary:\n",
    "        results_df = pd.DataFrame(all_results_summary)\n",
    "        print(\"\\n\\n--- Overall Dev Set Performance Summary ---\")\n",
    "        print(results_df[['model', 'dataset', 'accuracy', 'precision', 'recall', 'f1']])\n",
    "        print(\"\\n\\n--- Average Performance Across Dev Subsets (per model) ---\")\n",
    "        for col in ['accuracy', 'precision', 'recall', 'f1']:\n",
    "            results_df[col] = pd.to_numeric(results_df[col], errors='coerce')\n",
    "        avg_performance = results_df.groupby('model')[['accuracy', 'precision', 'recall', 'f1']].mean()\n",
    "        print(avg_performance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
